{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beYEzxo5J3mJ"
      },
      "outputs": [],
      "source": [
        "cd ~/ali.kilinc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqKJ20n9J3mN"
      },
      "outputs": [],
      "source": [
        "import pymysql\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import inspect\n",
        "import matplotlib.pyplot as plt\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', 40)\n",
        "pd.set_option('display.width', 120)\n",
        "from sqlalchemy import create_engine\n",
        "import datetime\n",
        "from datas import run_query_ai, create_engine_ai, create_engine_ai2, create_engine_ai3, run_query\n",
        "inspect.getargspec(run_query_ai)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kK0oG8PJ3mQ"
      },
      "outputs": [],
      "source": [
        "database = 'sm'\n",
        "table_to_use = \"sm.ret\"\n",
        "\n",
        "#your query\n",
        "query = \"\"\"select * from \"\"\" + table_to_use + \"\"\" where ;\"\"\"\n",
        "\n",
        "engine = create_engine_ai3(database)\n",
        "pre_df = run_query(query = query, con = engine)\n",
        "\n",
        "print(pre_df.head(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNH0s4RKJ3mS"
      },
      "outputs": [],
      "source": [
        "#we have a large amount of variables, so we need to handle them manually in preprocessing.\n",
        "#this csv file keeps variable names, their real dtypes and decide whether they should be discarded or not\n",
        "var_df = pd.read_csv('varlist_usage_v2.csv', na_values = '', delimiter = ',')\n",
        "print(var_df.head())\n",
        "var_df['name'] = var_df['name'].str.lower()\n",
        "\n",
        "print(var_df.loc[var_df['name'].isin(['a', 'b', 'c']), 'discard'])\n",
        "var_df.loc[var_df['name'].isin(['d', 'e', 'f']), 'discard'] = 1\n",
        "print(var_df.loc[var_df['name'].isin(['g', 'h', 'j']), 'discard'])\n",
        "\n",
        "var_df.loc[var_df['name'].isin(['y']), 'discard'] = 1\n",
        "var_df.loc[var_df['name'].isin(['z']), 'discard'] = 0\n",
        "\n",
        "var_df.loc[var_df['name'].isin(['k', 'l']), 'type'] = 'category'\n",
        "\n",
        "var_drop = list(var_df['name'][var_df['discard']==1])\n",
        "#print(var_drop)\n",
        "print(var_df.loc[var_df['name'].isin(['m']), 'type'])\n",
        "\n",
        "var_df.drop(var_df.loc[var_df['name'].isin(var_drop)==True].index, inplace = True)\n",
        "var_df.reset_index(drop=True, inplace = True)\n",
        "print(var_df.head())\n",
        "\n",
        "pre_df.replace('NA', np.nan, inplace = True)\n",
        "pre_df.replace('NULL', np.nan, inplace=True)\n",
        "pre_df.columns = pre_df.columns.str.lower()\n",
        "\n",
        "pre_df.drop(columns = var_drop, axis = 1, inplace = True)\n",
        "#var_df['type'] = np.where(war_df['name'].isin([xxxx]), 'category', var_df['type'])\n",
        "var_df['type'] = np.where(var_df['type'].isin(['int']), 'Int64', var_df['type'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACrQ1vmIJ3mV"
      },
      "outputs": [],
      "source": [
        "#cardinality check, it can also be automated\n",
        "print(pre_df.nunique().sort_values(ascending=False).head(20))\n",
        "print(len(pre_df))\n",
        "#print(variable5.value_counts().sort_values(ascendint=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhV61jaTJ3mW"
      },
      "outputs": [],
      "source": [
        "pre_df_copy = pre_df.copy()\n",
        "main_df = pre_df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBqCAmldJ3mY"
      },
      "outputs": [],
      "source": [
        "main_df_cols = main_df.columns\n",
        "varlist = list(var_df['name'].str.lower())\n",
        "\n",
        "var_df.loc[var_df['name'].isin(['k']), 'type'] = 'category'\n",
        "\n",
        "#changing python dtypes and if they are different than our list, change dtype. Float thing was not necessary, but after so many errors inside pipe, i needed to do it\n",
        "for i in range(len(main_df_cols)):\n",
        "    if var_df.loc[i, 'type'] == 'datetime64':\n",
        "        #main_df[main_df_cols[i]] = pd.to_datetime(main_df[main_df_cols[i]], format = var_df.loc[i, 'timetype'], errors = 'coerce')\n",
        "        pass\n",
        "    if var_df.loc[i, 'type'] == 'Int64':\n",
        "        print(main_df_cols[i])\n",
        "        print(var_df.loc[i,'type'])\n",
        "        main_df[main_df_cols[i]] = main_df[main_df_cols[i]].astype('float')\n",
        "        main_df[main_df_cols[i]] = main_df[main_df_cols[i]].astype(var_df.loc[i,'type'])\n",
        "    else:\n",
        "        print(main_df_cols[i])\n",
        "        print(var_df.loc[i, 'type'])\n",
        "        main_df[main_df_cols[i]] = main_df[main_df_cols[i]].astype(var_df.loc[i, 'type'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKUF1mwLJ3ma"
      },
      "outputs": [],
      "source": [
        "#setting a NA threshold and dropping vars according to that. Also fix, empty and duplicate columns are deleted.\n",
        "main_df.dropna(axis=1, thresh = int(0.40*(main_df.shape[0])), inplace = True)\n",
        "\n",
        "df_y = main_df[['y']]\n",
        "main_df.drop(columns = list(df_y.columns), axis = 1, inplace = True)\n",
        "\n",
        "fix_cols = []\n",
        "for col in main_df.columns:\n",
        "    if main_df[col].nunique() == 1:\n",
        "        fix_cols.append(col)\n",
        "    else:\n",
        "        pass\n",
        "    \n",
        "emp_cols = []\n",
        "for col in main_df.columns:\n",
        "    if main_df[col].count() > 0:\n",
        "        pass\n",
        "    else:\n",
        "        emp_cols.append(col)\n",
        "\n",
        "print(len(fix_cols))\n",
        "print(len(emp_cols))\n",
        "\n",
        "print(main_df.info())\n",
        "\n",
        "def getDuplicateColumns(df):\n",
        "    duplicateColumnNames = set()\n",
        "    duplicateColumnDict = dict()\n",
        "    for x in range(df.shape[1]):\n",
        "        col = df.iloc[:,x]\n",
        "        for y in range(x+1, df.shape[1]):\n",
        "            otherCol = df.iloc[:,y]\n",
        "            if col.equals(otherCol):\n",
        "                duplicateColumnNames.add(df.columns.values[y])\n",
        "                duplicateColumnDict[df.columns[x]] = df.columns[y]\n",
        "                \n",
        "    return list(duplicateColumnNames), duplicateColumnDict\n",
        "\n",
        "dupColNames, dupColDict = getDuplicateColumns(main_df)\n",
        "\n",
        "for key, value in dupColDict.items():\n",
        "    print(\"key = {}, value = {}\".format(key,value))\n",
        "dropcols = list()\n",
        "\n",
        "dropcols.extend([x for x in fix_cols if x not in dropcols])\n",
        "dropcols.extend([x for x in emp_cols if x not in dropcols])\n",
        "dropcols.extend([x for x in dupColNames if x not in dropcols])\n",
        "print(dropcols)\n",
        "\n",
        "main_df.drop(columns = dropcols, axis = 1, inplace = True)\n",
        "\n",
        "var_df = var_df[var_df['name'].isin(list(main_df.columns) + list(df_y.columns)) == True]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGXnVg47J3mc"
      },
      "outputs": [],
      "source": [
        "df_y_final = df_y['y']\n",
        "\n",
        "print(main_df.dtypes.astype(str).value_counts())\n",
        "\n",
        "date_cols = list(main_df.select_dtypes(include = ['datetime64']).columns)\n",
        "\n",
        "num_cols = list(main_df.select_dtypes(include = ['float', 'int', 'Int64', 'float64']).columns)\n",
        "cat_cols = list(main_df.select_dtypes(exclude = ['float', 'int', 'Int64', 'float64', 'datetime64']).columns)\n",
        "\n",
        "main_df.drop(columns = date_cols, axis = 1, inplace = True)\n",
        "\n",
        "print(len(num_cols))\n",
        "print(len(date_cols))\n",
        "print(len(cat_cols))\n",
        "\n",
        "main_df1 = main_df.iloc[:50000,]\n",
        "main_df2 = main_df.iloc[50000:,]\n",
        "print(main_df1.info())\n",
        "#!! category dtype do not let you replace or fill np.nan's with new category value, that is why i switched to str type\n",
        "#divided df into 2 as it was having memory error\n",
        "for col in cat_cols:\n",
        "    main_df1[col] = main_df1[col].astype('str')\n",
        "    main_df1[col].replace(np.nan, 'NULL', inplace = True)\n",
        "    \n",
        "for col in cat_cols:\n",
        "    main_df2[col] = main_df2[col].astype('str')\n",
        "    main_df2[col].replace(np.nan, 'NULL', inplace = True)\n",
        "    \n",
        "main_df = main_df1.append(main_df2, ignore_index = True)\n",
        "\n",
        "print(main_df.dtypes.astype(str).value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxlkajVaJ3me"
      },
      "outputs": [],
      "source": [
        "print(len(main_df))\n",
        "print(len(df_y_final))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCqAltFqJ3mf"
      },
      "outputs": [],
      "source": [
        "from statistics import mean\n",
        "\n",
        "def get_redundant_pairs(df):\n",
        "    pairs_to_drop = set()\n",
        "    cols = df.columns\n",
        "    for i in range(0,df.shape[1]):\n",
        "        for j in range(0,i+1):\n",
        "            pairs_to_drop.add((cols[i], cols[j]))\n",
        "    return pairs_to_drop\n",
        "\n",
        "def get_triangle(df):\n",
        "    au_corr = df.corr().abs().unstack()\n",
        "    labels_to_drop = get_redundant_pairs(df)\n",
        "    au_corr = au_corr.drop(labels=labels_to_drop)\n",
        "    #print(au_corr)\n",
        "    au_corr = au_corr.sort_values(ascending=False)\n",
        "    return au_corr\n",
        "\n",
        "left_cols = num_cols.copy()\n",
        "corr_cols = []\n",
        "elim_rows = []\n",
        "mean_dict = {}\n",
        "row = 0\n",
        "while get_triangle(main_df[left_cols])[0:1].values > 0.6:\n",
        "    \n",
        "    #print(len(left_cols))\n",
        "    sub_df = get_triangle(main_df[left_cols])\n",
        "    filter_df_1 = [t for t in sub_df.index if (t[0] == sub_df.index[0][0]) or (t[1] == sub_df.index[0][0])]\n",
        "    filter_df_2 = [t for t in sub_df.index if (t[0] == sub_df.index[0][1]) or (t[1] == sub_df.index[0][1])]\n",
        "    #print(np.nanmean(sub_df[filter_df_1].values))\n",
        "    if np.nanmean(sub_df[filter_df_1].values) > np.nanmean(sub_df[filter_df_2].values):\n",
        "        corr_cols.append(sub_df[0:1].index[0][0])\n",
        "        left_cols.remove(sub_df[0:1].index[0][0])\n",
        "    else:\n",
        "        corr_cols.append(sub_df[0:1].index[0][1])\n",
        "        left_cols.remove(sub_df[0:1].index[0][1])\n",
        "    elim_rows.append(sub_df[0:1])\n",
        "    mean_dict[row]=[(sub_df[0:1].index[0][0], np.nanmean(sub_df[filter_df_1].values)), (sub_df[0:1].index[0][1], np.nanmean(sub_df[filter_df_2].values))]\n",
        "    row = row + 1\n",
        "    #print(len(left_cols))\n",
        "\n",
        "print(corr_cols)\n",
        "print(elim_rows)\n",
        "print(mean_dict)\n",
        "\n",
        "num_cols = [x for x in num_cols if x not in corr_cols]\n",
        "#print(sub_df[sub_df.apply(lambda x: 'curr_revenue_amt_pre_cr' in x)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAd7nMNHJ3mh"
      },
      "outputs": [],
      "source": [
        "print(num_cols)\n",
        "main_df.drop(corr_cols, axis=1, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWbkacyLJ3mh"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.inspection import permutation_importance\n",
        "from eli5.sklearn import PermutationImportance\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder, MinMaxScaler\n",
        "from sklearn.feature_selection import RFECV\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.feature_selection import GenericUnivariateSelect\n",
        "\n",
        "dt = DecisionTreeRegressor(random_state = 0)\n",
        "lgb = LGBMClassifier(random_state = 23, max_depth = 6, n_estimators = 300)\n",
        "\n",
        "main_df[cat_cols].replace(np.nan, 'NULL', inplace=True)\n",
        "orden = OrdinalEncoder()\n",
        "main_df[cat_cols] = orden.fit_transform(main_df[cat_cols])\n",
        "main_df[num_cols] = main_df[num_cols].fillna(main_df[num_cols].median())\n",
        "\n",
        "print(main_df.head())\n",
        "print(df_y_final.head())\n",
        "pi = PermutationImportance(dt, cv=3,random_state = 0)\n",
        "\n",
        "\n",
        "#rfecv has a problem with pipeline recognition\n",
        "cat_transformer = Pipeline(steps = [('enc', OrdinalEncoder())])\n",
        "num_transformer = Pipeline(steps = [('sc1', MinMaxScaler(feature_range = (0,1)))])\n",
        "preprocessor = ColumnTransformer(transformers = [('cat', 'passthrough', cat_cols), ('num', 'passthrough', num_cols)], remainder = 'passthrough')\n",
        "\n",
        "all_pipe = Pipeline(steps = [('prep', preprocessor), ('est', lgb)])\n",
        "\n",
        "rfe = RFECV(estimator = pi, cv=3, min_features_to_select = 80, scoring = 'neg_mean_squared_error', step = 2, n_jobs =4)\n",
        "rfe.fit(main_df, df_y_final)\n",
        "\n",
        "\"\"\"\n",
        "mutual_inf = mutual_info_classif(main_df, df_y_final, random_state = 10)\n",
        "generic = GenericUnivariateSelect(score_func = mutual_info_classif, mode = 'k_best', param=100)\n",
        "main_df_trans = generic.fit_transform(main_df, df_y_final)\n",
        "print(main_df['var8'][main_df['var8'].isnull()])\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqOUbwuYJ3mj"
      },
      "outputs": [],
      "source": [
        "main_df[cat_cols] = orden.inverse_transform(main_df[cat_cols])\n",
        "cols_left = main_df.columns[rfe.get_support()].values\n",
        "df_left = pd.DataFrame(main_df, columns = cols_left)\n",
        "print(cols_left)\n",
        "print(len(cols_left))\n",
        "num_vars = []\n",
        "cat_vars = []\n",
        "for col in cols_left:\n",
        "    if col in cat_cols:\n",
        "        df_left[col] = df_left[col].astype('object')\n",
        "        cat_vars.append(col)\n",
        "    else:\n",
        "        pass\n",
        "    \n",
        "for col in cols_left:\n",
        "    if col in num_cols:\n",
        "        df_left[col] = df_left[col].astype('float')\n",
        "        num_vars.append(col)\n",
        "    else:\n",
        "        pass\n",
        "    \n",
        "print(num_vars)\n",
        "print(cat_vars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbKNukMWJ3ml"
      },
      "outputs": [],
      "source": [
        "def col_ind_get(prep, all_cols, num_cols, cat_cols, enc_cols):\n",
        "    \n",
        "    pre_all_cols = []\n",
        "    pre_cat_cols = []\n",
        "    pre_cat_ind = []\n",
        "    \n",
        "    cat_cols = list(set(cat_cols) - set(enc_cols))\n",
        "    \n",
        "    for i in range(len(prep.transformers)):\n",
        "        \n",
        "        pre_all_cols.extend(prep.transformers[i][2])\n",
        "        \n",
        "    pre_cat_cols = [item for item in pre_all_cols if item in cat_cols]\n",
        "    pre_cat_ind = [i for i in range(len(pre_all_cols)) if pre_all_cols[i] in cat_cols]\n",
        "    pre_num_cols = [item for item in pre_all_cols if item in num_cols]\n",
        "    pre_num_ind = [i for i in range(len(pre_all_cols)) if pre_all_cols[i] in num_cols]\n",
        "    \n",
        "    return pre_all_cols, pre_cat_cols, pre_cat_ind, pre_num_cols, pre_num_ind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zrg_YAJzJ3ml"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from catboost import CatBoostRegressor, Pool, FeaturesData\n",
        "\n",
        "print(cat_vars)\n",
        "print(num_vars)\n",
        "ordered_cols = cat_vars + num_vars\n",
        "\n",
        "df_y_final = df_y['y']\n",
        "\n",
        "df_left2 = df_left.reindex(columns = ordered_cols)\n",
        "\n",
        "test_list = []\n",
        "\n",
        "for col in cat_vars:\n",
        "    df_left2[col]=df_left2[col].astype('str').str.replace('.0','')\n",
        "\n",
        "for col in cat_vars:\n",
        "    #print(col)\n",
        "    cont = df_left2[col][df_left2[col].astype('str').str.contains('35.0')]\n",
        "    #print(cont)\n",
        "    if len(cont) > 0:\n",
        "        test_list.append(cont)\n",
        "\n",
        "\n",
        "feature_selection = 1\n",
        "enc_vars = []\n",
        "\n",
        "if feature_selection == 1:\n",
        "    models = dict()\n",
        "    paramscat = {\n",
        "        #'est__depth': np.linspace(8,8,1,endpoint=True),\n",
        "        'est__depth': [8],\n",
        "        'est__iterations': [600],\n",
        "        'est__learning_rate': [0.01],\n",
        "        #'est__bagging_temperature': [10,20],\n",
        "        #'est__border_count': [75],\n",
        "        #'est__loss_function': ['MultiClass'],\n",
        "        #'est__eval_metric': ['AUC']\n",
        "    }\n",
        "\n",
        "models['catb'] = [CatBoostRegressor(verbose = 0), paramscat]\n",
        "\n",
        "for key, value in models.items():\n",
        "    \n",
        "    scorer = ['neg_mean_squared_error', 'neg_mean_absolute_error', 'r2']\n",
        "    \n",
        "    numeric_transformer = Pipeline(steps = [('scl', MinMaxScaler(feature_range = (0,1)))])\n",
        "    preprocessor = ColumnTransformer(transformers = [('cat', 'passthrough', cat_vars),\n",
        "                                                    ('num', 'passthrough', num_vars)], remainder = 'passthrough')\n",
        "    \n",
        "    all_pipe = Pipeline(steps = [('prep', preprocessor), ('est', value[0])])\n",
        "    \n",
        "    pre_all_cols, pre_cat_cols, pre_cat_ind, pre_num_cols, pre_num_ind = col_ind_get(preprocessor, df_left2.columns, num_vars, cat_vars, enc_vars)\n",
        "    search_space = value[1]\n",
        "    print(pre_all_cols)\n",
        "    print(pre_cat_cols)\n",
        "    print(pre_cat_ind)\n",
        "    print(pre_num_cols)\n",
        "    print(pre_num_ind)\n",
        "    print(list(df_left2.columns))\n",
        "    print(df_left2.columns)\n",
        "    print(df_left2.head(2))\n",
        "    print(df_y_final.head(5))\n",
        "    grid_search = GridSearchCV(all_pipe, search_space, cv= 5, verbose = 1, refit = 'neg_mean_squared_error', scoring = scorer, return_train_score = True, n_jobs=4)\n",
        "    grid_search.fit(df_left2, y=df_y_final,est__cat_features=pre_cat_ind)\n",
        "    \n",
        "ind = grid_search.best_index_\n",
        "print(\"model = {}\".format(key))\n",
        "print(\"train_mse = {}, test_mse = {}\".format(grid_search.cv_results_['mean_train_neg_mean_squared_error'][ind], grid_search.cv_results_['mean_test_neg_mean_squared_error'][ind]))\n",
        "print(\"train_mae = {}, test_mae = {}\".format(grid_search.cv_results_['mean_train_neg_mean_absolute_error'][ind], grid_search.cv_results_['mean_test_neg_mean_absolute_error'][ind]))\n",
        "print(\"train_r2 = {}, test_r2 = {}\".format(grid_search.cv_results_['mean_train_r2'][ind], grid_search.cv_results_['mean_test_r2'][ind]))\n",
        "print(\"avg_fit_time = {}\".format(grid_search.cv_results_['mean_fit_time'][ind]))\n",
        "print(\"best_params = {}\".format(grid_search.best_params_))\n",
        "print(\"best_estimator = {}\".format(grid_search.best_estimator_.named_steps['est']))\n",
        "\n",
        "best_params = dict()\n",
        "best_params[key] = grid_search.best_params_\n",
        "\n",
        "best_results = dict()\n",
        "best_results[key] = [grid_search.cv_results_['mean_test_neg_mean_squared_error'][ind], grid_search.cv_results_['mean_test_neg_mean_absolute_error'][ind], grid_search.cv_results_['mean_test_r2'][ind]]\n",
        "\n",
        "importances = grid_search.best_estimator_.named_steps['est'].feature_importances_\n",
        "importances = [float(i) for i in importances]\n",
        "\n",
        "print(pre_all_cols)\n",
        "print(importances)\n",
        "print(list(zip(pre_all_cols, importances)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmaRBkrsJ3mn"
      },
      "outputs": [],
      "source": [
        "imp_df = pd.DataFrame(columns = ['col', 'imp'])\n",
        "imp_df['col'] = pre_all_cols\n",
        "imp_df['imp'] = importances\n",
        "imp_df['dtype'] = np.where(imp_df['col'].isin(num_vars), 'num','cat')\n",
        "imp_df.sort_values(by = ['imp'], ascending = False, inplace = True, ignore_index = True)\n",
        "print(imp_df.head(5))\n",
        "\n",
        "top_imp_df = imp_df.iloc[0:60,]\n",
        "\n",
        "print(top_imp_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6jWgzXYJ3mo"
      },
      "outputs": [],
      "source": [
        "import pandas_profiling as pp\n",
        "\n",
        "filtered_df = df_left2[list(top_imp_df['col'])]\n",
        "\n",
        "df_y_report = pd.DataFrame(df_y_final, columns = ['y'])\n",
        "print(df_y_report)\n",
        "report_df = pd.concat([filtered_df, df_y_report], axis = 1)\n",
        "report_df = pd.DataFrame(report_df)\n",
        "report_df['y'] = report_df['y'].astype('object')\n",
        "print(report_df.info())\n",
        "\n",
        "report.to_file('report_new.html')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "colab": {
      "name": "Regressor.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}